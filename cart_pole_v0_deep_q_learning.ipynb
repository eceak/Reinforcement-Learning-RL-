{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkyATxWVMULyMsGdClAd2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eceak/Reinforcement-Learning-RL-/blob/main/cart_pole_v0_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2j3fCcjE3lW",
        "outputId": "1a988927-13c7-40af-cec4-7b07cf184030"
      },
      "source": [
        "pip install keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOk6E4X9E-ZZ",
        "outputId": "e8173715-8d4d-4eff-98cf-20cb88ff1186"
      },
      "source": [
        "pip install Adam"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Adam\n",
            "  Downloading adam-0.0.0.dev0-py2.py3-none-any.whl (2.6 kB)\n",
            "Installing collected packages: Adam\n",
            "Successfully installed Adam-0.0.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CKqjSsDE-hs",
        "outputId": "2f3ce494-9f65-4e7d-a945-d4a1149ee4c3"
      },
      "source": [
        "\"\"\"\n",
        "@author: ece\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "'''deep q learning kullanıldığı için keras \n",
        "kütüphaneleri aktif edildi'''\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "class DQLAgent:\n",
        "     \n",
        "    def __init__(self, env):\n",
        "        #parameter/hyperparameter\n",
        "       \n",
        "      ###DQN girdisi state_size kadar nöron olduğu için ve \n",
        "      ### action_size kadar da çıktı nöron olacağı \n",
        "      ###için bunların bilinmesi gerekir.\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        \n",
        "        self.gamma = 0.95 #gelecekteki ödülleri belirtir (future reward)\n",
        "        self.learning_rate = 0.001 #agentın öğrenme hızı\n",
        "        \n",
        "        self.epsilon = 1 #explore\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        \n",
        "        self.memory = deque(maxlen = 1000) #kapasitesi 1000 olan bir liste hafıza için yaratıldı\n",
        "        \n",
        "        self.model = self.build_model()\n",
        "    \n",
        "    def build_model(self):\n",
        "        #neural network for deep q learning\n",
        "        model = Sequential()\n",
        "        model.add(Dense(48,input_dim = self.state_size,activation = \"tanh\"))\n",
        "        model.add(Dense(self.action_size,activation = \"linear\"))\n",
        "        model.compile(loss = \"mse\", optimizer = Adam(lr = self.learning_rate))\n",
        "        return model\n",
        "    \n",
        "    def remember(self,state, action, reward, next_state, done):\n",
        "        #storage\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def act(self, state):\n",
        "        #acting explore or exploit\n",
        "        if random.uniform(0,1) <= self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            act_values = self.model.predict(state)\n",
        "            return np.argmax(act_values[0])\n",
        "    \n",
        "    def replay(self,batch_size):\n",
        "        #training\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
        "            train_target = self.model.predict(state)\n",
        "            train_target[0][action] = target\n",
        "            self.model.fit(state,train_target, verbose = 0)\n",
        "        \n",
        "    \n",
        "    def adaptiveEGreedy(self):\n",
        "        if self.epsilon > self.epsilon_min :\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #initialize gym env and agent\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    agent = DQLAgent(env)\n",
        "    \n",
        "    batch_size = 16\n",
        "    episodes = 100\n",
        "    for e in range(episodes):\n",
        "        \n",
        "        #initialize environment\n",
        "        state = env.reset() \n",
        "        \n",
        "        #ilerleyen adımlarda stateleri bir arada tutmak için reshape kullanılır\n",
        "        state = np.reshape(state, [1,4])\n",
        "        \n",
        "        time = 0\n",
        "        while True:\n",
        "            \n",
        "            #act / bir hareket seçiliyor\n",
        "            action = agent.act(state) #select an action\n",
        "            \n",
        "            #step\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1,4])\n",
        "            \n",
        "            #remember/storage\n",
        "            agent.remember(state,action, reward, next_state, done)\n",
        "            \n",
        "            #update state\n",
        "            state = next_state\n",
        "            \n",
        "            #replay\n",
        "            agent.replay(batch_size)\n",
        "            \n",
        "            #adjust epsilon\n",
        "            agent.adaptiveEGreedy()\n",
        "            \n",
        "            time += 1\n",
        "            \n",
        "            if done:\n",
        "                print(\"Episode: {}, time: {}\".format(e,time))\n",
        "                break\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, time: 45\n",
            "Episode: 1, time: 15\n",
            "Episode: 2, time: 11\n",
            "Episode: 3, time: 34\n",
            "Episode: 4, time: 35\n",
            "Episode: 5, time: 16\n",
            "Episode: 6, time: 10\n",
            "Episode: 7, time: 11\n",
            "Episode: 8, time: 10\n",
            "Episode: 9, time: 9\n",
            "Episode: 10, time: 15\n",
            "Episode: 11, time: 37\n",
            "Episode: 12, time: 17\n",
            "Episode: 13, time: 49\n",
            "Episode: 14, time: 16\n",
            "Episode: 15, time: 31\n",
            "Episode: 16, time: 41\n",
            "Episode: 17, time: 64\n",
            "Episode: 18, time: 45\n",
            "Episode: 19, time: 69\n",
            "Episode: 20, time: 53\n",
            "Episode: 21, time: 42\n",
            "Episode: 22, time: 30\n",
            "Episode: 23, time: 41\n",
            "Episode: 24, time: 45\n",
            "Episode: 25, time: 53\n",
            "Episode: 26, time: 35\n",
            "Episode: 27, time: 42\n",
            "Episode: 28, time: 47\n",
            "Episode: 29, time: 54\n",
            "Episode: 30, time: 39\n",
            "Episode: 31, time: 44\n",
            "Episode: 32, time: 37\n",
            "Episode: 33, time: 36\n",
            "Episode: 34, time: 99\n",
            "Episode: 35, time: 57\n",
            "Episode: 36, time: 77\n",
            "Episode: 37, time: 200\n",
            "Episode: 38, time: 200\n",
            "Episode: 39, time: 200\n"
          ]
        }
      ]
    }
  ]
}